{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1560af4af866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[4]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark-lecture'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "    .master('local[4]') \\\n",
    "    .appName('spark-lecture') \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import random\n",
    "\n",
    "n=100  #n times\n",
    "\n",
    "heads = (sc.parallelize(xrange(n))\n",
    "        .map(lambda _: random.random())\n",
    "        .filter(lambda r: r<0.5)\n",
    "        .count())\n",
    "        \n",
    "# we must always parallelize --> xrange(n) is because we want to parallelize to n\n",
    "# _: from every number that came from the above line; we map each number to random.random() using lambda function\n",
    "# we just want to filter things lower than 0.5\n",
    "\n",
    "tails = n - heads\n",
    "ratio = 1. * heads /n\n",
    "\n",
    "print(\"heads: \", heads)\n",
    "print(\"tails: \", tails)\n",
    "print(\"ratio: \", ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def is_prime(number):\n",
    "    factor_min = 2\n",
    "    factor_max = int(number**0.5)+1\n",
    "    for factor in xrange(factor_min, factor_max):\n",
    "        if number % factor == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "#defines if a number is prime or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "numbers = xrange(2,100)\n",
    "\n",
    "primes = (sc.parallelize(numbers)\n",
    "        .filter(is_prime)  # filter with the function\n",
    "        .collect())\n",
    "\n",
    "print(primes)\n",
    "\n",
    "\n",
    "#or\n",
    "# rdd = sc.parallelize(numbers)\n",
    "# rdd2 = rdd.filter(is_prime)\n",
    "# rdd3 = rdd2.filter(lambda x: x>5)\n",
    "# rdd3.collect()  --> collects them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "sc.parallelize([1,3,2,3,2,4,5]).distinct().collect()         # outputs [1,2,3,4,5] --> distinct numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "sc.parallelize([1,3,2,3,2]).sortBy(lambda x: x, ascending=False).collect()         #outputs [3,3,2,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "## MAP AND FLAT MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "%%writefile input.txt         #file name\n",
    "hello world\n",
    "another line\n",
    "yet another line\n",
    "yet another another line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "rdd = sc.textFile('input.txt')         # parallelizes\n",
    "rdd.collect()                          # outputs by line\n",
    "\n",
    "#but if we do ...\n",
    "rdd2 = rdd.map(lambda x: x.split())           # splits elements by space, it is a transformation\n",
    "rdd2.collect()                                # outputs words --> a list of lists: every sentence is a list, composed with each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "rdd3 = rdd.flatMap(lambda x: x.split())\n",
    "rdd3.collect()                                # outputs words but not as a list of lists (only a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "%%writefile sales.txt\n",
    "# writes something here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "sc.textFile('sales.txt').top(2)               # gets the top 2 rows excluding header\n",
    "sc.textFile('sales.txt').take(2)              # gets the top 2 rows counting header (1: the header, 2: the first row) --> output is a list of 2 rows - it should be split\n",
    "\n",
    "sc.textFile('sales.txt'). \\\n",
    "    map(lambda x: x.split()). \\\n",
    "    take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# to remove the first 2 rows which do not start with «, e.g.\n",
    "sc.textFile('sales.txt'). \\\n",
    "    map(lambda x: x.split()). \\\n",
    "    filter(lambda x: x not x[0].startswith('«')). \\\n",
    "    take(2)\n",
    "    \n",
    "# if we did not write \"not\" we would get the 2 or less rows which start with «"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# to get the 2 first rows of the last column\n",
    "sc.textFile('sales.txt').\\\n",
    "    map(lambda x: x.split()).\\\n",
    "    filter(lambda x: x not x[0].startswith('«')).\\\n",
    "    map(lambda x: x[-1]).\\\n",
    "    take(2)                                                 #outputs a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# to sum the 2 first rows of the last column\n",
    "sc.textFile('sales.txt').\\\n",
    "    map(lambda x: x.split()).\\\n",
    "    filter(lambda x: x not x[0].startswith('«')).\\\n",
    "    map(lambda x: float(x[-1])).\\                           # converts to float since the list gives something as u'element'\n",
    "    take(2)                                                 # outputs a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "sc.textFile('sales.txt').\\\n",
    "    map(lambda x: x.split()).\\\n",
    "    filter(lambda x: x not x[0].startswith('«')).\\\n",
    "    map(lambda x: (x[3], float(x[-1]))).\\                           # two columns\n",
    "    collect()                                                       # outputs a list of tuples [()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "sc.textFile('sales.txt').\\\n",
    "    map(lambda x: x.split()).\\\n",
    "    filter(lambda x: x not x[0].startswith('«')).\\\n",
    "    map(lambda x: (x[3], float(x[-1]))).\\                           # two columns\n",
    "    reduceByKey(lambda amount1, amount2: amount1+amount2).\\         # the input is a tuple, we group the results by amount 1 (the key) and amount2 has a transformation (sums all elements of the same key after grouping)\n",
    "    collect()                                                       # outputs a list of tuples [()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "sc.textFile('sales.txt').\\\n",
    "    map(lambda x: x.split()).\\\n",
    "    filter(lambda x: x not x[0].startswith('«')).\\\n",
    "    map(lambda x: (x[3], float(x[-1]))).\\                           # two columns\n",
    "    reduceByKey(lambda amount1, amount2: amount1+amount2).\\         # the input is a tuple, we group the results by amount 1 (the key) and amount2 has a transformation (sums all elements of the same key after grouping)\n",
    "    sortBy(lambda state_amount: state_amount[1], ascending = False).\\   # sorts by the second column in a descending order\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "## SPARK-ML --> chain transformers and estimators together to compose ML pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark as ps\n",
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "spark = ps.sql.SparkSession.builder.\\\n",
    "          .master('local[2]').\\\n",
    "          .appName('spark-ml').\\\n",
    "          .getOrCreate()\n",
    "            \n",
    "sc = spark.sparkContext\n",
    "\n",
    "sqlContext = SQLContext(sc)               # wrap the SQLContext in a spark context to collect the SQL Context to use the SQL functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df_aapl = sqlContext.read.csv('data/aapl.csv',\n",
    "                   header = True,                   #use header or not\n",
    "                   quote = '\"',                     #char for quotes\n",
    "                   sep = ',',                       #char for separation\n",
    "                   inferSchema = True)              #do we infer Schema or not?\n",
    "\n",
    "df_aapl.show(5)                                     # similar to df.head(5), but shows the headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df_aapl.schema      #similar to df.info() --> tells the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df_out = df_aapl.select('Date', 'Close').orderBy('Close', ascending=False)       # uses SQL commands, selects the 'Date' and 'Close' columns, grouping them by 'Close' in a descending order\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "# assemble values in a vector --> VectorAssembler is a transformer\n",
    "vectorAssembler = VectorAssembler (inputCols = ['Close'], outputCols='Features')           # forms a new column with the values of 'Close' as a vector (list)\n",
    "\n",
    "df_vector = vectorAssembler.transform(df_aapl)        \n",
    "df_aapl.show(5)                                       # shows the original table\n",
    "df_vector.show(5)                                     # shows the modified table (with the new column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "scaler = MinMaxScaler(inputCol = \"Features\", outputCol = \"Scaled Features\")                # forms a new column with the standardized values of \"Features\"\n",
    "\n",
    "#compute summary statistics and generate MinMaxScalerModel\n",
    "scaler_model = scaler.fit(df_vector)\n",
    "\n",
    "#rescale each feature to range[min, max]\n",
    "scalar_data = scaler.model.transform(df_vector)\n",
    "scalar_data.select(\"Features\", \"Scaled Features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "## TRANSFORMERS, ESTIMATORS, PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression               #estimator\n",
    "from pyspark.ml.feature import RegexTokenizer, HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Prepare training documents from a list of (id, text, label) tuples\n",
    "\n",
    "training= spark.createDataFrame([\n",
    "        (0, \"spark is like hadoop mapreduce\", 1.0),\n",
    "        (1, \"sparks light fire!!!\", 0.0),\n",
    "        (2, \"elephants like simba\", 0.0),\n",
    "        (3, \"hadoop is an elephant\", 1.0),\n",
    "        (4, \"hadoop mapreduce\", 1.0)\n",
    "    ], ['id', 'text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol = 'text', outputCol = \"tokens\", pattern =\"\\\\W\")   # it is a transformer; \\\\W splits by widespace\n",
    "hashingTF = HashingTF(inputCol = \"tokens\", outputCol = \"features\")\n",
    "lr = LogisticRegression(maxIter = 10, regParam = 0.001)\n",
    "\n",
    "tokens = regexTokenizer.transform(training)         # transform the original df to give us the tokenized words\n",
    "hashes = hashingTF.transform(tokens)\n",
    "logistic_model = lr.fit(hashes)               # uses columns named features / label by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Prepare test documents, which are unlabeled (id, text) tuples\n",
    "test= spark.createDataFrame([\n",
    "        (5, \"simba has a spark\"),\n",
    "        (6, \"hadoop\"),\n",
    "        (7, \"mapreduce in spark\"),\n",
    "        (8, \"apache hadoop\")\n",
    "    ], ['id', 'text'])\n",
    "\n",
    "# What do we need to do to this to make a prediction?\n",
    "preds = logistic_model.transform(hashingTF.transform(regexTokenizer.transform(test)))\n",
    "preds.select('text', 'prediction', 'probability').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# cols=list(df.columns)\n",
    "#I = [2, 5, 7, 8, 10, 14]\n",
    "#new_cols = np.delete(cols, I).tolist()\n",
    "#df = df[new_cols]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}