{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Twitter API authentication\n",
    "\n",
    "import tweepy\n",
    "\n",
    "api_key =  # api_key\n",
    "api_secret_key = # api_secret_key\n",
    "access_token =  # access_token\n",
    "access_token_secret =  # access_token_secret\n",
    "\n",
    "# authorize the API Key\n",
    "authentication = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "\n",
    "# authorization to user's access token and access token secret\n",
    "authentication.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# call the api\n",
    "api = tweepy.API(authentication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Streaming tweets from home timeline\n",
    "\n",
    "public_tweet = api.home_timeline(count=5)\n",
    "\n",
    "for tweet in public_tweet:\n",
    "    print(\"-->\",tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Streaming tweets from user timeline\n",
    "\n",
    "user = \"AnalyticsVidhya\"\n",
    "public_tweet = api.user_timeline(id=user,count=5)\n",
    "\n",
    "for tweet in public_tweet:\n",
    "    print(\"-->\",tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Retrieve tweets\n",
    "result = api.search(['covid','Covid-19','COVID-19'], lang='en', count=10)\n",
    "\n",
    "# JSON keys\n",
    "pprint(result[0]._json.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self, time_limit=300):\n",
    "        self.start_time = time.time()\n",
    "        self.limit = time_limit\n",
    "        super(MyStreamListener, self).__init__()\n",
    "    \n",
    "    def on_connect(self):\n",
    "        print(\"Connected to Twitter API.\")\n",
    "        \n",
    "    def on_status(self, status):\n",
    "        \n",
    "        \n",
    "        # Tweet ID\n",
    "        tweet_id = status.id\n",
    "        \n",
    "        # User ID\n",
    "        user_id = status.user.id\n",
    "        # Username\n",
    "        username = status.user.name\n",
    "        \n",
    "        \n",
    "        # Tweet\n",
    "        if status.truncated == True:\n",
    "            tweet = status.extended_tweet['full_text']\n",
    "            hashtags = status.extended_tweet['entities']['hashtags']\n",
    "        else:\n",
    "            tweet = status.text\n",
    "            hashtags = status.entities['hashtags']\n",
    "        \n",
    "        # Read hastags\n",
    "        hashtags = read_hashtags(hashtags)            \n",
    "        \n",
    "        # Retweet count\n",
    "        retweet_count = status.retweet_count\n",
    "        # Language\n",
    "        lang = status.lang\n",
    "        \n",
    "        \n",
    "        # If tweet is not a retweet and tweet is in English\n",
    "        if not hasattr(status, \"retweeted_status\") and lang==\"en\":\n",
    "            # Connect to database\n",
    "            dbConnect(user_id, username, tweet_id, tweet, retweet_count, hashtags)\n",
    "            \n",
    "        if (time.time() - self.start_time) > self.limit:\n",
    "            \n",
    "            print(time.time(), self.start_time, self.limit)\n",
    "            return False\n",
    "            \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            # Returning False in on_data disconnects the stream\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Extract hashtags\n",
    "def read_hashtags(tag_list):\n",
    "    hashtags = []\n",
    "    for tag in tag_list:\n",
    "        hashtags.append(tag['text'])\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Table creation\n",
    "commands = (# Table 1\n",
    "            '''Create Table TwitterUser(User_Id BIGINT PRIMARY KEY, User_Name TEXT);''',\n",
    "            # Table 2\n",
    "            '''Create Table TwitterTweet(Tweet_Id BIGINT PRIMARY KEY,\n",
    "                                         User_Id BIGINT,\n",
    "                                         Tweet TEXT,\n",
    "                                         Retweet_Count INT,\n",
    "                                         CONSTRAINT fk_user\n",
    "                                             FOREIGN KEY(User_Id)\n",
    "                                                 REFERENCES TwitterUser(User_Id));''',\n",
    "            # Table 3\n",
    "            '''Create Table TwitterEntity(Id SERIAL PRIMARY KEY,\n",
    "                                         Tweet_Id BIGINT,\n",
    "                                         Hashtag TEXT,\n",
    "                                         CONSTRAINT fk_user\n",
    "                                             FOREIGN KEY(Tweet_Id)\n",
    "                                                 REFERENCES TwitterTweet(Tweet_Id));''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connection to database server\n",
    "conn = psycopg2.connect(host=\"localhost\",database=\"TwitterDB\",port=5432,user=<user>,password=<password>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Create cursor to execute SQL commands\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute SQL commands\n",
    "for command in commands:\n",
    "    # Create tables\n",
    "    cur.execute(command)\n",
    "\n",
    "# Close communication with server\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Insert Tweet data into database\n",
    "def dbConnect(user_id, user_name, tweet_id, tweet, retweet_count, hashtags):\n",
    "    \n",
    "    conn = psycopg2.connect(host=\"localhost\",database=\"TwitterDB\",port=5432,user=<user>,password=<password>)\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # insert user information\n",
    "    command = '''INSERT INTO TwitterUser (user_id, user_name) VALUES (%s,%s) ON CONFLICT\n",
    "                 (User_Id) DO NOTHING;'''\n",
    "    cur.execute(command,(user_id,user_name))\n",
    "\n",
    "    # insert tweet information\n",
    "    command = '''INSERT INTO TwitterTweet (tweet_id, user_id, tweet, retweet_count) VALUES (%s,%s,%s,%s);'''\n",
    "    cur.execute(command,(tweet_id, user_id, tweet, retweet_count))\n",
    "    \n",
    "    # insert entity information\n",
    "    for i in range(len(hashtags)):\n",
    "        hashtag = hashtags[i]\n",
    "        command = '''INSERT INTO TwitterEntity (tweet_id, hashtag) VALUES (%s,%s);'''\n",
    "        cur.execute(command,(tweet_id, hashtag))\n",
    "    \n",
    "    # Commit changes\n",
    "    conn.commit()\n",
    "    \n",
    "    # Disconnect\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Streaming tweets\n",
    "\n",
    "myStreamListener = MyStreamListener()\n",
    "myStream = tweepy.Stream(auth=api.auth, listener=myStreamListener,\n",
    "                        tweet_mode=\"extended\")\n",
    "myStream.filter(track=['Covid','covid-19'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Cleaning the tweets\n",
    "def preprocess(tweet):\n",
    "    \n",
    "    # remove links\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    # remove mentions\n",
    "    tweet = re.sub(\"@\\w+\",\"\",tweet)\n",
    "    # alphanumeric and hashtags\n",
    "    tweet = re.sub(\"[^a-zA-Z#]\",\" \",tweet)\n",
    "    # remove multiple spaces\n",
    "    tweet = re.sub(\"\\s+\",\" \",tweet)\n",
    "    tweet = tweet.lower()\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sent = ' '.join([lemmatizer.lemmatize(w) for w in tweet.split() if len(lemmatizer.lemmatize(w))>3])\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Connecting to the Database\n",
    "def DbConnect(query):\n",
    "    \n",
    "    conn = psycopg2.connect(host=\"localhost\",database=\"TwitterDB\",port=5432,user=<user>,password=<password>)\n",
    "    curr = conn.cursor()\n",
    "    \n",
    "    curr.execute(query)\n",
    "    \n",
    "    rows = curr.fetchall()\n",
    "    \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Create \n",
    "data_tweet = DbConnect(\"SELECT User_Id, Tweet_Id, Tweet FROM TwitterTweet;\")\n",
    "\n",
    "df_tweet = pd.DataFrame(columns=['User_Id','Tweet_Id','Clean_Tweet'])\n",
    "\n",
    "for data in data_tweet:\n",
    "    index = len(df_tweet)\n",
    "    df_tweet.loc[index,'User_Id'] = data[0]\n",
    "    df_tweet.loc[index,'Tweet_Id'] = data[1]\n",
    "    df_tweet.loc[index,'Clean_Tweet'] = preprocess(data[2])\n",
    "    \n",
    "df_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Querying hashtags from database\n",
    "data_tags = DbConnect(\"SELECT Tweet_Id, Hashtag FROM TwitterEntity;\")\n",
    "\n",
    "df_tags = pd.DataFrame(columns=['Tweet_Id','Hashtags'])\n",
    "\n",
    "for data in data_tags:\n",
    "    index = len(df_tags)\n",
    "    df_tags.loc[index,'Tweet_Id'] = data[0]\n",
    "    df_tags.loc[index,'Hashtags'] = data[1]\n",
    "    \n",
    "df_tags.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from collections import Counter\n",
    "import nltk                                              # used for preprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from nltk.stem import SnowballStemmer\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "#from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "##from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df = pd.read_csv(\"fake_or_real_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  "
      ]
     },
     "execution_count": 4,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# how do we turn a corpus of a text (list of texts / documents) into a feature matrix?\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "corpus = list(df.title)    # list of documents will be the news heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# cleanse the data from stopwords and punctuation: we split words (x.split() is a list of words) by space from df.msg, ''.join allows to join words as total string again\n",
    "\n",
    "stopwords_set = set(stopwords)\n",
    "punctuation_set = set(string.punctuation)\n",
    "df['cleaned_title'] = df.title.apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords_set and word not in punctuation_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillary’s fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "\n",
       "                                       cleaned_title  \n",
       "0                       you can smell hillary’s fear  \n",
       "1  watch the exact moment paul ryan committed pol...  "
      ]
     },
     "execution_count": 10,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_title'] = df.cleaned_title.str.lower()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# streams the code to use it many times and avoid repetition --> we pass a list of tuples into the pipeline\n",
    "# preprocessing : takes one column (each cell is a clean version of the title), then we fit through TfidfVectorizer to vectorize it --> each word becomes a column (feature)\n",
    "pipeline = Pipeline([('count_vect', CountVectorizer(stop_words = stopwords_set)), \\\n",
    "                    #('tfidf', TfidfVectorizer(stop_words = stopwords_set)), \\\n",
    "                    ('lg', LogisticRegression())])   # spam data is fit into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8061868686868687\n",
      "[[629 156]\n",
      " [151 648]]\n"
     ]
    }
   ],
   "source": [
    "X = df.cleaned_title   # we pass the cleaned title to the pipeline\n",
    "y = df.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(pipeline.score(X_test, y_test))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df_final = pd.read_csv(\"twitter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X_twitter = df_final.title\n",
    "predictions = lg.predict(X_twitter)\n",
    "df_final['Predictions'] = predictions\n",
    "for value in df_final.shape[1]:\n",
    "    if predictions[value] >= 0.50:\n",
    "        df_final['Label'][value] == 'Likely True'\n",
    "    else:\n",
    "        df_final['Label'][value] == 'Likely False'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}